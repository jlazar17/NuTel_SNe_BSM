{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb35940-4a6d-412f-9c68-47dbd81e8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ae82f5-02f0-493e-8f87-170de60a700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"./paper.mplstyle\")\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import ridder\n",
    "\n",
    "# Set up ASTERIA\n",
    "import sys\n",
    "ASTERIA_PATH = \"/Users/jlazar/research/ASTERIA\"\n",
    "os.environ[\"ASTERIA\"] = \"/Users/jlazar/research/ASTERIA\"\n",
    "sys.path.append(f\"{ASTERIA_PATH}/python/\")\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sne_bsm import units, parameterized_flux_from_h5, deserialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2e35e7-4b2d-4492-82e1-542c7ea0debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infiles = glob(\"../data/majoron/from_yingying/*.csv\")\n",
    "# for infile in tqdm(infiles):\n",
    "#     outfile = f\"./data/{infile.split('/')[-1].replace('csv', 'npy')}\"\n",
    "#     a = np.genfromtxt(infile, delimiter=\",\")\n",
    "#     a[:, 2] = a[:, 2] / 6\n",
    "#     np.save(outfile, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc656c4d-c2cc-4a3f-b344-3a006516d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infile = \"./dphi-dEdt-100MeV-gn11dot8-100s-nf.csv\"\n",
    "# outfile = \"./data/majoran_fluxes.h5\"\n",
    "\n",
    "# if not os.path.exists(outfile):\n",
    "#     with h5.File(outfile, \"w\") as _:\n",
    "#         pass\n",
    "\n",
    "# for infile in infiles:\n",
    "#     key = infile.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "#     infile = f\"./data/{infile.split('/')[-1].replace('csv', 'npy')}\"\n",
    "#     arr = np.load(infile)\n",
    "#     times = np.sort(np.unique(arr[:, 0])) * units[\"second\"]\n",
    "#     energies = np.sort(np.unique(arr[:, 1])) * units[\"MeV\"]\n",
    "#     fluxes = np.empty(times.shape + energies.shape + (3,),)\n",
    "\n",
    "#     for idx in range(len(times)):\n",
    "#         flux = arr[idx*len(energies):(idx+1)*len(energies), 2] / units[\"MeV\"] / units[\"second\"]\n",
    "#         flux = np.where(flux >= 0, flux, 0) # Sometimes numerical issues can sneak in to give negative fluxes\n",
    "#         fluxes[idx, :, 0] = flux # nue\n",
    "#         fluxes[idx, :, 1] = flux # nuebar\n",
    "#         fluxes[idx, :, 2] = flux # nux per flavor\n",
    "\n",
    "#     with h5.File(outfile, \"r+\") as h5f:\n",
    "\n",
    "#         if key in h5f.keys():\n",
    "#             del h5f[key]\n",
    "\n",
    "#         h5f.create_group(key)\n",
    "#         h5f[key].create_dataset(\"energies\", data=energies)\n",
    "#         h5f[key].create_dataset(\"times\", data=times)\n",
    "#         h5f[key].create_dataset(\"fluxes\", data=fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee31e78c-b31f-45c1-ae9f-fc87c03e81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = \"sm_flux\"\n",
    "# nue_arr = np.genfromtxt(\"./fluxactivenuE.csv\", delimiter=\",\")\n",
    "# nuebar_arr = np.genfromtxt(\"./fluxactivenuEBar.csv\", delimiter=\",\")\n",
    "# nux_arr = np.genfromtxt(\"./fluxactivenuX.csv\", delimiter=\",\")\n",
    "\n",
    "# times = np.sort(np.unique(nue_arr[:, 0]))\n",
    "# energies = np.sort(np.unique(nue_arr[:, 1]))\n",
    "\n",
    "# if not np.all(times==np.sort(np.unique(nuebar_arr[:, 0]))):\n",
    "#     raise ValueError()\n",
    "# if not np.all(times==np.sort(np.unique(nux_arr[:, 0]))):\n",
    "#     raise ValueError()\n",
    "# if not np.all(energies==np.sort(np.unique(nuebar_arr[:, 1]))):\n",
    "#     raise ValueError()\n",
    "# if not np.all(energies==np.sort(np.unique(nux_arr[:, 1]))):\n",
    "#     raise ValueError()\n",
    "    \n",
    "# fluxes = np.empty(times.shape + energies.shape + (3,),)\n",
    "\n",
    "# for idx in range(len(times)):\n",
    "#     nue_flux = nue_arr[idx*len(energies):(idx+1)*len(energies), 2] / units[\"MeV\"] / units[\"second\"]\n",
    "#     nuebar_flux = nuebar_arr[idx*len(energies):(idx+1)*len(energies), 2] / units[\"MeV\"] / units[\"second\"]\n",
    "#     nux_flux = nux_arr[idx*len(energies):(idx+1)*len(energies), 2] / units[\"MeV\"] / units[\"second\"] / 4\n",
    "#     # Sometimes numerical issues can sneak in to give negative fluxes\n",
    "#     nue_flux = np.where(nue_flux >= 0, nue_flux, 0)\n",
    "#     nuebar_flux = np.where(nuebar_flux >= 0, nuebar_flux, 0)\n",
    "#     nux_flux = np.where(nux_flux >= 0, nux_flux, 0)\n",
    "#     fluxes[idx, :, 0] = nue_flux # nue\n",
    "#     fluxes[idx, :, 1] = nuebar_flux # nuebar\n",
    "#     fluxes[idx, :, 2] = nux_flux # nux per flavor\n",
    "    \n",
    "    \n",
    "# if not os.path.exists(\"./data/sm_flux.h5\"):\n",
    "#     with h5.File(\"./data/sm_flux.h5\", \"w\") as _:\n",
    "#         pass\n",
    "    \n",
    "# with h5.File(\"./data/sm_flux.h5\", \"r+\") as h5f:\n",
    "\n",
    "#         if key in h5f.keys():\n",
    "#             del h5f[key]\n",
    "\n",
    "#         h5f.create_group(key)\n",
    "#         h5f[key].create_dataset(\"energies\", data=energies)\n",
    "#         h5f[key].create_dataset(\"times\", data=times)\n",
    "#         h5f[key].create_dataset(\"fluxes\", data=fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4858b648-36df-4d8a-9477-b69b144a20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"sm_flux\"\n",
    "infile = \"data/sm_flux.h5\"\n",
    "serialize_file = \"./data/serialized_sm_flux.h5\"\n",
    "\n",
    "thin = 1\n",
    "\n",
    "done_keys = []\n",
    "if os.path.exists(serialize_file):\n",
    "    with h5.File(serialize_file) as h5f:\n",
    "        done_keys = list(h5f.keys())\n",
    "\n",
    "with h5.File(infile, \"r\") as h5f:\n",
    "    for key, group in h5f.items():\n",
    "        if f\"{key}_0\" in done_keys:\n",
    "            continue\n",
    "        pflux = parameterized_flux_from_h5(group, 10*units.kpc, thin=thin)\n",
    "        pflux.serialize(serialize_file, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd8fa0a-0051-4925-a2e1-35f721ec587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:00<00:00, 3387.84it/s]\n"
     ]
    }
   ],
   "source": [
    "infile = \"data/majoran_fluxes.h5\"\n",
    "serialize_file = \"./data/serialized_majoran_fluxes.h5\"\n",
    "\n",
    "thin = 10\n",
    "\n",
    "done_keys = []\n",
    "if os.path.exists(serialize_file):\n",
    "    with h5.File(serialize_file) as h5f:\n",
    "        done_keys = list(h5f.keys())\n",
    "\n",
    "with h5.File(infile, \"r\") as h5f:\n",
    "    for key, group in tqdm(h5f.items()):\n",
    "        if f\"{key}_0\" in done_keys:\n",
    "            continue\n",
    "        pflux = parameterized_flux_from_h5(group, 10*units.kpc, thin=thin, track=False)\n",
    "        pflux.serialize(serialize_file, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c336bd-09ef-44af-a866-d2df11409ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMAX = 100 * units[\"second\"]\n",
    "TMIN = 1e-2 * units[\"second\"]\n",
    "TMINS = np.logspace(np.log10(TMIN), np.log10(TMAX), 201)\n",
    "DTS = np.linspace(0.001, 5, 401) * units[\"second\"]\n",
    "def find_significance_matrix(sig_hits, sm_hits, bg_hits, times, tmins=TMINS, dts=DTS):\n",
    "    significance = np.full(tmins.shape + dts.shape, np.nan)\n",
    "\n",
    "    for idx, tmin in enumerate(tmins):\n",
    "        for jdx, dt in enumerate(dts):\n",
    "            m = np.logical_and(tmin < times, times < tmin+dt)\n",
    "            if not m.sum():\n",
    "                continue\n",
    "            significance[idx, jdx] = likelihood(sig_hits[m], sm_hits[m], bg_hits[m])\n",
    "    return significance, tmins, dts\n",
    "\n",
    "def likelihood(sig_hits, sm_hits, bg_hits):\n",
    "    n_obs = (bg_hits + sm_hits).sum()\n",
    "    n_exp = (sig_hits + bg_hits + sm_hits).sum()\n",
    "    llh = 2 * (n_exp - n_obs)\n",
    "    if n_obs > 0:\n",
    "        llh += 2 * n_obs * np.log(n_obs / n_exp)\n",
    "    return llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "673e3130-54c2-4e28-950d-c601d224a756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlazar/miniconda3/envs/ic_sn_hnl/lib/python3.10/site-packages/snewpy/models/base.py:305: RuntimeWarning: divide by zero encountered in log\n",
      "  np.exp(np.log(L) - (2+a)*np.log(Ea) + (1+a)*np.log(1+a)\n",
      "/Users/jlazar/miniconda3/envs/ic_sn_hnl/lib/python3.10/site-packages/snewpy/models/base.py:305: RuntimeWarning: invalid value encountered in subtract\n",
      "  np.exp(np.log(L) - (2+a)*np.log(Ea) + (1+a)*np.log(1+a)\n",
      "/Users/jlazar/miniconda3/envs/ic_sn_hnl/lib/python3.10/site-packages/snewpy/models/base.py:306: RuntimeWarning: divide by zero encountered in divide\n",
      "  - loggamma(1+a) + a*np.log(E) - (1+a)*(E/Ea)) / (u.erg * u.s)\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '0d01'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m tqdm(h5f\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m     12\u001b[0m     key_split \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     mass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey_split\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mass \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[1;32m     15\u001b[0m         out[mass] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '0d01'"
     ]
    }
   ],
   "source": [
    "with h5.File(\"data/serialized_sm_flux.h5\", \"r\") as h5f:\n",
    "    sm_flux = deserialize(h5f[\"sm_flux_0\"])\n",
    "    \n",
    "sm_t, sm_hits = sm_flux.get_hits(\n",
    "    tmax=100 * units[\"second\"],\n",
    "    model_file=\"majoran.txt\"\n",
    ")\n",
    "out = {}\n",
    "\n",
    "with h5.File(\"data/serialized_majoran_fluxes.h5\", \"r\") as h5f:\n",
    "    for key in tqdm(h5f.keys()):\n",
    "        key_split = key.split(\"_\")[0].split(\"-\")\n",
    "        mass = float(key_split[2].replace(\"MeV\", \"\"))\n",
    "        if mass not in out:\n",
    "            out[mass] = []\n",
    "        coupling = float(f\"-{key_split[3][2:].replace('dot', '.')}\")\n",
    "        flux = deserialize(h5f[key])\n",
    "        \n",
    "        bsm_t, bsm_hits = flux.get_hits(\n",
    "            model_file=\"majoran.txt\",\n",
    "            tmax=100 * units[\"second\"]\n",
    "        )\n",
    "        \n",
    "        bg_hits = flux.get_background(\n",
    "            shape=bsm_hits.shape,\n",
    "            model_file=\"majoran.txt\",\n",
    "            tmax=100 * units[\"second\"]\n",
    "        )\n",
    "        \n",
    "        if np.any(sm_t!=bsm_t):\n",
    "            raise ValueError(\"Hit times are different !\")\n",
    "            \n",
    "        significance, _, _ = find_significance_matrix(bsm_hits, sm_hits, bg_hits, bsm_t)\n",
    "        m = ~np.isnan(significance)\n",
    "        out[mass].append((coupling, significance[m].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195058e-ce04-4c7a-9bfc-6f79003b1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in out.items():\n",
    "    v = sorted(v)\n",
    "    _, ax = plt.subplots()\n",
    "    \n",
    "    couplings = np.power(10, [x[0] for x in v])\n",
    "    significnces = [x[1] for x in v]\n",
    "    \n",
    "    plot_sens = True\n",
    "    try:\n",
    "        interp = interp1d(np.log(couplings), np.log(significnces))\n",
    "        f = lambda lc: np.exp(interp(lc)) - 3.841\n",
    "        res = ridder(f, np.log(couplings[0]), np.log(couplings[-1]))\n",
    "    except ValueError:\n",
    "        plot_sens = False\n",
    "    \n",
    "    # print(res)\n",
    "    \n",
    "    ax.plot(couplings, significnces)\n",
    "    ax.scatter(couplings, significnces)\n",
    "    if plot_sens:\n",
    "        ax.scatter([np.exp(res)], [np.exp(interp(res))], marker=\"*\", zorder=10, s=200, label=\"Sensitivity\")\n",
    "    ax.loglog()\n",
    "    ax.axhline(3.841)\n",
    "    \n",
    "    ax.set_title(f\"{int(k)} MeV Majoran\", fontsize=20)\n",
    "    ax.set_xlabel(r\"$g~\\left[?\\right]$\")\n",
    "    ax.set_ylabel(r\"$\\Delta \\mathrm{LLH}$\")\n",
    "    \n",
    "    ax.axhline(3.841, label=r\"$\\Delta\\mathrm{LLH}=3.84$\")\n",
    "    plt.savefig(f\"./figures/majoran_sensitivities_{int(k)}MeV.pdf\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d30ed-954e-42bf-9c21-74b8c743830e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ic_sn_hnl",
   "language": "python",
   "name": "ic_sn_hnl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
